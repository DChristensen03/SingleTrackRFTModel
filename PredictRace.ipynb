{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_abbreviation = \"cd\"\n",
    "track_name = \"Churchill Downs\"\n",
    "race_date = \"20250626\"\n",
    "scratches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_quality_dict = {\n",
    "    \"TRL\": 1,\n",
    "    \"MCL\": 1,\n",
    "    \"WMC\": 1,\n",
    "    \"MOC\": 1.5,\n",
    "    \"MSA\": 1.5,\n",
    "    \"MSW\": 2.5,\n",
    "    \"WCL\": 2,\n",
    "    \"CLM\": 2,\n",
    "    \"MST\": 2.5,\n",
    "    \"CLH\": 2.5,\n",
    "    \"CST\": 2.5,\n",
    "    \"SOC\": 2.5,\n",
    "    \"OCL\":\t2.75,\n",
    "    \"SHP\":\t3,\n",
    "    \"STR\":\t2.75,\n",
    "    \"AOC\": 3.25,\n",
    "    \"OCS\": 3.5,\n",
    "    \"OCH\":\t3.25,\n",
    "    \"ALW\":\t4,\n",
    "    \"HCP\":\t4,\n",
    "    \"SIM\":\t2,\n",
    "    \"SST\":\t3.5,\n",
    "    \"STK\": 5\n",
    "}\n",
    "\n",
    "race_types = {\n",
    "    \"AL\": \"ALW\",\n",
    "    \"MS\": \"MSW\",\n",
    "    \"CL\": \"CLM\",\n",
    "    \"OC\": \"AOC\",\n",
    "    \"MC\": \"MCL\",\n",
    "    \"SO\": \"SOC\",\n",
    "    \"MO\": \"MCL\", # MO is Maiden Optional Claiming but the simulcast data contains no definition for it\n",
    "    \"ST\": \"STK\",\n",
    "    \"SA\": \"STR\"\n",
    "}\n",
    "\n",
    "equipment = {\n",
    "    \"B\": \"B\",\n",
    "    \"F\": \"None\",\n",
    "    \"BF\": \"B\",\n",
    "    \"V\": \"V\",\n",
    "    \"R\": \"R\",\n",
    "    \"Y\": \"Y\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PP data for:  cd20250626ppsXML.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 2: Load and Parse XML Data using pandas.read_xml\n",
    "def load_performance_data(file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    print(\"Loading PP data for: \", os.path.basename(file_path))\n",
    "    \n",
    "    # Extract each Race element within EntryRaceCard and convert to a dictionary\n",
    "    races = []\n",
    "    for race in root.findall('.//racedata'):\n",
    "        race_dict = xmltodict.parse(ET.tostring(race))['racedata']\n",
    "        race_date = race_dict['race_date']\n",
    "        track_name = race_dict['track']\n",
    "\n",
    "        race_dict = extract_general_race_info(race_dict, race_date, track_name)\n",
    "\n",
    "        num_scratches = 0\n",
    "        for entry in race.findall('.//horsedata'):\n",
    "            entry_dict = extract_entry_info(entry, race_date, num_scratches)\n",
    "            workout_dict = extract_workout_info(entry, race_date)\n",
    "            \n",
    "            is_scratched = False\n",
    "            for horse in scratches:\n",
    "                if horse.lower() in entry_dict['horse_id'].split('_')[0].lower():\n",
    "                    print(horse, entry_dict['horse_id'].split('_')[0])\n",
    "                    is_scratched = True\n",
    "                    num_scratches += 1\n",
    "                    break\n",
    "            if not is_scratched:\n",
    "                races.append({**race_dict, **entry_dict, **workout_dict})\n",
    "        \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(races)\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_general_race_info(race_dict, race_date, track_name):\n",
    "    race_id = f\"{race_date}_{race_dict['race']}_{track_name}\"\n",
    "    return {\n",
    "        \"race_id\": race_id,\n",
    "        \"course_type\": str(race_dict['surface']),\n",
    "        \"distance\": int(float(race_dict['distance'])),\n",
    "        \"race_type\": race_types[str(race_dict['stkorclm'])],\n",
    "        \"restriction_type\": \"S\" if \"state\" in str(race_dict['race_text']).lower() else \"None\",\n",
    "        \"purse\": float(race_dict['purse']),\n",
    "        \"number_of_run\": len(race_dict['horsedata']),\n",
    "    }\n",
    "\n",
    "def extract_entry_info(entry_root, race_date, scratches):\n",
    "    entry_dict = xmltodict.parse(ET.tostring(entry_root))['horsedata']\n",
    "\n",
    "    final_dict = {\n",
    "        \"horse_id\": f\"{entry_dict['horse_name']}_{entry_dict['program']}\",\n",
    "        \"gender\": str(entry_dict['sex']),\n",
    "        \"post_position\": int(entry_dict['pp']) - scratches,\n",
    "        \"weight\": int(entry_dict['weight']),\n",
    "        \"equipment\": str(entry_dict['equip']),\n",
    "        \"medication\": \"L\" if str(entry_dict['med']) == \"1\" else \"None\",\n",
    "        \"jockey_win_percentage\": float(entry_dict['jockey']['stats_data']['stat']['wins']) / float(entry_dict['jockey']['stats_data']['stat']['starts']) if float(entry_dict['jockey']['stats_data']['stat']['starts']) != 0 else 0,\n",
    "        \"trainer_win_percentage\": float(entry_dict['trainer']['stats_data']['stat']['wins']) / float(entry_dict['trainer']['stats_data']['stat']['starts']) if float(entry_dict['trainer']['stats_data']['stat']['starts']) != 0 else 0,\n",
    "        \"trainer_jockey_win_percentage\": float(entry_dict['stats_data']['stat'][22]['wins']) / float(entry_dict['stats_data']['stat'][22]['starts']) if float(entry_dict['stats_data']['stat'][22]['starts']) != 0 else 0,\n",
    "    }\n",
    "\n",
    "    summaries = entry_root.findall('.//stats_data')\n",
    "    if summaries is list:\n",
    "        summary_dict = xmltodict.parse(ET.tostring(summaries[0]))['stat']['THIS_YEAR']\n",
    "        final_dict.update({\n",
    "            \"win_percentage_year\": float(summary_dict['wins']) / float(summary_dict['starts']),\n",
    "            \"otb_percentage_year\": (float(summary_dict['wins']) + float(summary_dict['places']) + float(summary_dict['shows'])) / float(summary_dict['starts']),\n",
    "        })\n",
    "\n",
    "    ignored_scratches = 0\n",
    "    for i, pp in enumerate(entry_root.findall('.//ppdata')):\n",
    "        normalized_i = i - ignored_scratches\n",
    "        if normalized_i > 5:\n",
    "            break\n",
    "        pp_dict = xmltodict.parse(ET.tostring(pp))['ppdata']\n",
    "\n",
    "        # Get the actual equipment being worn\n",
    "        if normalized_i == 0:\n",
    "            if final_dict['equipment'] != \"OFF\":\n",
    "                if str(pp_dict['equipment']) in equipment.keys():\n",
    "                    final_dict.update({\n",
    "                        \"equipment\": equipment[str(pp_dict['equipment'])]\n",
    "                    })\n",
    "                else:\n",
    "                    final_dict.update({\n",
    "                        \"equipment\": \"None\"\n",
    "                    })\n",
    "            else:\n",
    "                final_dict.update({\n",
    "                    \"equipment\": \"O\"\n",
    "                })\n",
    "\n",
    "        race_type = str(pp_dict['racetype'])\n",
    "        if (race_type == 'SCR'):\n",
    "            ignored_scratches += 1\n",
    "            continue\n",
    "        race_quality = race_quality_dict[race_type] if str(race_type) in race_quality_dict.keys() else 1\n",
    "\n",
    "        if str(pp_dict['statebredr']) == 'S':\n",
    "            race_quality -= 1\n",
    "        if race_type == 'STK' and pp_dict['racegrade'] != 0:\n",
    "            race_quality += 1 + int(pp_dict['racegrade'])\n",
    "\n",
    "        if i == 0:\n",
    "            final_dict.update({\n",
    "                \"pp_layoff\": (datetime.strptime(race_date, '%Y%m%d') - datetime.strptime(pp_dict['racedate'][:10], '%Y%m%d')).days\n",
    "            })\n",
    "\n",
    "        bad_luck = False\n",
    "        long_comment = str(pp_dict['longcommen']).lower()\n",
    "        if long_comment is not None:\n",
    "            if any(['bump' in long_comment, 'stumbled' in long_comment, 'checked' in long_comment, 'steadied' in long_comment, 'stopped' in long_comment, 'squeezed' in long_comment, 'steady' in long_comment or 'steadied' in long_comment, 'head turned' in long_comment, 'unprepared start' in long_comment, 'wd' in long_comment or 'wide' in long_comment, 'bled' in long_comment]):\n",
    "                bad_luck = True\n",
    "\n",
    "        final_dict.update({\n",
    "            f\"pp_track_{normalized_i}\": str(pp_dict['trackcode']),\n",
    "            f\"pp_time_since_race_{normalized_i}\": (datetime.strptime(race_date, '%Y%m%d') - datetime.strptime(pp_dict['racedate'][:10], '%Y%m%d')).days,\n",
    "            f\"pp_course_type_{normalized_i}\": str(pp_dict['surface']),\n",
    "            f\"pp_distance_{normalized_i}\": int(pp_dict['distance']),\n",
    "            f\"pp_quality_{normalized_i}\": race_quality,\n",
    "            f\"pp_purse_{normalized_i}\": float(pp_dict['purse']),\n",
    "            f\"pp_normalized_position_{normalized_i}\": np.divide(float(pp_dict['positionfi']), float(pp_dict['fieldsize'])),\n",
    "            f\"pp_class_rating_{normalized_i}\": int(pp_dict['classratin']),\n",
    "            f\"pp_speed_rating_{normalized_i}\": int(pp_dict['speedfigur']),\n",
    "            f\"pp_pace_figure_{normalized_i}\": int(pp_dict['pacefigur2']),\n",
    "            f\"pp_bad_luck_{normalized_i}\": bad_luck,\n",
    "        })\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def extract_workout_info(entry_root, race_date):\n",
    "    final_dict = {}\n",
    "    for i, workout in enumerate(entry_root.findall('.//workoutdata')):    \n",
    "        if i > 3:\n",
    "            return final_dict\n",
    "        workout_dict = xmltodict.parse(ET.tostring(workout))['workoutdata']\n",
    "        final_dict.update({\n",
    "            f\"workout_last_month_{i}\": True if int(workout_dict['days_back']) < 30 else False,\n",
    "            f\"workout_distance_{i}\": int(workout_dict['worktext'][0]) * 100,\n",
    "            f\"workout_course_type_{i}\": \"D\", # No data available, usually dirt\n",
    "            f\"workout_time_{i}\": int(float(re.sub('\\D', '', get_substring_from_char(workout_dict['worktext'], ':'))) * 10),\n",
    "            f\"workout_rank_{i}\": int(workout_dict['ranking']) / int(workout_dict['rank_group']),\n",
    "        })\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def get_substring_from_char(s, char):\n",
    "    pos = s.find(char) + 1\n",
    "    if pos != -1:\n",
    "        return s[pos:]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "performance_path = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\XML_PPs\"\n",
    "file_name = f'{track_abbreviation}{race_date}ppsXML.xml'  # Add your suffixes here\n",
    "\n",
    "# Load all past performance files\n",
    "performance_data = load_performance_data(performance_path + '\\\\' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_21960\\3537981293.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  performance_data.at[i, \"first_off_layoff\"] = False\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_21960\\3537981293.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  performance_data.at[i, \"second_off_layoff\"] = False\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_21960\\3537981293.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  performance_data.at[i, \"third_off_layoff\"] = False\n"
     ]
    }
   ],
   "source": [
    "# Go through each row and create new columns\n",
    "for i, row in performance_data.iterrows():\n",
    "    # Initialize new columns\n",
    "    performance_data.at[i, \"first_off_layoff\"] = False\n",
    "    performance_data.at[i, \"second_off_layoff\"] = False\n",
    "    performance_data.at[i, \"third_off_layoff\"] = False\n",
    "    \n",
    "    # Set if horse is off layoff\n",
    "    if row['pp_time_since_race_0'] > 45:\n",
    "        performance_data.at[i, 'first_off_layoff'] = True\n",
    "    elif row['pp_time_since_race_1'] - row['pp_time_since_race_0'] > 45:\n",
    "        performance_data.at[i, 'second_off_layoff'] = True\n",
    "    elif row['pp_time_since_race_2'] - row['pp_time_since_race_1'] > 45:\n",
    "        performance_data.at[i, 'third_off_layoff'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encoding column: restriction_type\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "performance_data.reset_index()\n",
    "\n",
    "# Drop unneeded columns\n",
    "horse_ids = performance_data['horse_id']\n",
    "performance_data = performance_data.drop(columns=['horse_id'])\n",
    "\n",
    "# Impute only columns with missing values\n",
    "imputer: SimpleImputer = pickle.load(open(f\"Models\\\\{track_name}\\\\imputer.pkl\", \"rb\"))\n",
    "columns_with_missing = imputer.feature_names_in_\n",
    "imputed_array = imputer.transform(performance_data[columns_with_missing])\n",
    "\n",
    "# Convert the imputed array back to a DataFrame with original column names\n",
    "imputed_data = pd.DataFrame(imputed_array, columns=columns_with_missing)\n",
    "\n",
    "# Combine the imputed columns with the rest of the data\n",
    "data = performance_data.copy()\n",
    "data[columns_with_missing] = imputed_data\n",
    "\n",
    "# Get correct column types\n",
    "data = data.infer_objects()\n",
    "\n",
    "# Use LabelEncoder on string columns\n",
    "label_encoders = pickle.load(open(f\"Models\\\\{track_name}\\\\label_encoders.pkl\", \"rb\"))\n",
    "for col in data.columns:\n",
    "    if col == \"race_id\" or col == \"horse_id\":\n",
    "        data[col] = LabelEncoder().fit_transform(data[col])\n",
    "    elif data[col].dtype == 'object':\n",
    "        try:\n",
    "            if ('pp_track' in col):\n",
    "                # If the track was unknown in test, set it to be the same as the track we're running at\n",
    "                data[col] = data[col].map(lambda s: track_abbreviation.upper() if s not in label_encoders[col].classes_ else s)\n",
    "            data[col] = label_encoders[col].transform(data[col])\n",
    "        except:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col])\n",
    "            print(f\"Error encoding column: {col}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ensemble of 5 models\n",
      "Ensemble predictions generated with confidence scores\n",
      "Average confidence: 56.28\n",
      "Confidence range: 0.00 - 100.00\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open(f\"Models\\\\{track_name}\\\\{track_name}_Model.pkl\", \"rb\"))\n",
    "\n",
    "feature_names = model.feature_names_in_\n",
    "\n",
    "# Reorder the columns of merged_data_imputed to match the order of feature_names\n",
    "data = data[feature_names]\n",
    "\n",
    "# Try to load ensemble models first, fallback to single model\n",
    "try:\n",
    "    ensemble_models = pickle.load(open(f\"Models\\\\{track_name}\\\\{track_name}_Ensemble.pkl\", \"rb\"))\n",
    "    print(f\"Loaded ensemble of {len(ensemble_models)} models\")\n",
    "    use_ensemble = True\n",
    "    model = ensemble_models[0]  # Keep for compatibility\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensemble models not found, using single model\")\n",
    "    model = pickle.load(open(f\"Models\\\\{track_name}\\\\{track_name}_Model.pkl\", \"rb\"))\n",
    "    use_ensemble = False\n",
    "\n",
    "feature_names = model.feature_names_in_\n",
    "\n",
    "# Reorder the columns of merged_data_imputed to match the order of feature_names\n",
    "data = data[feature_names]\n",
    "\n",
    "# Generate predictions\n",
    "if use_ensemble:\n",
    "    # Generate predictions from all models in the ensemble\n",
    "    ensemble_predictions = []\n",
    "    for i, ensemble_model in enumerate(ensemble_models):\n",
    "        predictions = ensemble_model.predict(data)\n",
    "        ensemble_predictions.append(predictions)\n",
    "    \n",
    "    # Convert to numpy array for easier manipulation\n",
    "    ensemble_predictions = np.array(ensemble_predictions)\n",
    "    \n",
    "    # Calculate ensemble statistics\n",
    "    y_predict = np.mean(ensemble_predictions, axis=0)  # Mean prediction\n",
    "    prediction_std = np.std(ensemble_predictions, axis=0)  # Standard deviation across models\n",
    "    \n",
    "    # Calculate confidence scores based on model agreement (inverse of standard deviation)\n",
    "    # Lower std = higher confidence, higher std = lower confidence\n",
    "    max_std = np.max(prediction_std)\n",
    "    min_std = np.min(prediction_std)\n",
    "    \n",
    "    # Normalize confidence to 0-100 scale (100 = highest confidence, 0 = lowest confidence)\n",
    "    if max_std > min_std:\n",
    "        confidence_scores = 100 * (1 - (prediction_std - min_std) / (max_std - min_std))\n",
    "    else:\n",
    "        confidence_scores = np.full_like(prediction_std, 100)  # All predictions have same confidence\n",
    "    \n",
    "    print(f\"Ensemble predictions generated with confidence scores\")\n",
    "    print(f\"Average confidence: {np.mean(confidence_scores):.2f}\")\n",
    "    print(f\"Confidence range: {np.min(confidence_scores):.2f} - {np.max(confidence_scores):.2f}\")\n",
    "    \n",
    "else:\n",
    "    # Single model prediction\n",
    "    y_predict = model.predict(data)\n",
    "    confidence_scores = np.full_like(y_predict, 50)  # Default moderate confidence\n",
    "    print(\"Single model predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated for 80 horses\n",
      "Confidence scores range: 0.0 to 100.0\n"
     ]
    }
   ],
   "source": [
    "# Use the ensemble predictions from the previous cell\n",
    "predicted_normalized_position = y_predict\n",
    "predicted_finish_position = ((predicted_normalized_position * data['number_of_run']) / 100)\n",
    "\n",
    "# Add confidence scores to the results\n",
    "print(f\"Predictions generated for {len(predicted_finish_position)} horses\")\n",
    "print(f\"Confidence scores range: {np.min(confidence_scores):.1f} to {np.max(confidence_scores):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame by concatenating the series, now including confidence scores\n",
    "results_df = pd.concat([performance_data['race_id'], horse_ids, predicted_finish_position, pd.Series(confidence_scores, index=performance_data.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Churchill Downs on 20250626:\n",
      "============================================================\n",
      "\n",
      "Race 1:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #4 PIERCE ELEVATED (Predicted: 3.15, Confidence: 52.3 - MODERATE)\n",
      "  2. #6 GOLDEN PLATE (Predicted: 3.28, Confidence: 78.4 - HIGH)\n",
      "  3. #3 GOLD SEARCH (Predicted: 3.31, Confidence: 59.3 - MODERATE)\n",
      "  4. #2 GLOBAL LEGEND (Predicted: 3.42, Confidence: 48.7 - LOW)\n",
      "  Spread: 0.13, Avg Top-2 Confidence: 65.3\n",
      "----------------------------------------\n",
      "\n",
      "Race 2:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #3 BORN FLASHY (Predicted: 2.51, Confidence: 0.0 - LOW)\n",
      "  2. #4 BOURBON OUTLAW (Predicted: 2.92, Confidence: 40.7 - LOW)\n",
      "  3. #6 MUGATU (Predicted: 3.41, Confidence: 9.5 - LOW)\n",
      "  4. #7 SHATTUCK (Predicted: 3.92, Confidence: 62.2 - MODERATE)\n",
      "  Spread: 0.41, Avg Top-2 Confidence: 20.4\n",
      "----------------------------------------\n",
      "\n",
      "Race 3:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #5 ASHES AND DIAMONDS (Predicted: 3.12, Confidence: 100.0 - HIGH)\n",
      "  2. #6 ORDER RESTORED (Predicted: 3.52, Confidence: 53.2 - MODERATE)\n",
      "  3. #4 DARK THIRTY (Predicted: 3.58, Confidence: 30.6 - LOW)\n",
      "  4. #8 DARTY TIME (Predicted: 3.67, Confidence: 2.8 - LOW)\n",
      "  Spread: 0.40, Avg Top-2 Confidence: 76.6\n",
      "----------------------------------------\n",
      "\n",
      "Race 4:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #5 LORD BULLINGDON (Predicted: 3.61, Confidence: 69.6 - MODERATE)\n",
      "  2. #1 BOURBON RESOLVE (Predicted: 3.70, Confidence: 95.2 - HIGH)\n",
      "  3. #9 SANTORINI (Predicted: 4.46, Confidence: 92.8 - HIGH)\n",
      "  4. #3 OUT OF DEDUCTIONS (Predicted: 4.52, Confidence: 44.6 - LOW)\n",
      "  Spread: 0.09, Avg Top-2 Confidence: 82.4\n",
      "----------------------------------------\n",
      "\n",
      "Race 5:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #4 EXECUTIVE ACTION (Predicted: 3.64, Confidence: 69.9 - MODERATE)\n",
      "  2. #1 DEAN MARTINI (Predicted: 4.25, Confidence: 49.1 - LOW)\n",
      "  3. #2 EL REY DORADO (Predicted: 4.26, Confidence: 30.0 - LOW)\n",
      "  4. #3 NORWICH (Predicted: 4.30, Confidence: 18.4 - LOW)\n",
      "  Spread: 0.61, Avg Top-2 Confidence: 59.5\n",
      "----------------------------------------\n",
      "\n",
      "Race 6:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #2 LADY OF SILENCE (Predicted: 3.25, Confidence: 56.0 - MODERATE)\n",
      "  2. #1 CUE THE DUCKBOATS (Predicted: 3.26, Confidence: 85.3 - HIGH)\n",
      "  3. #4 CHASTEN (Predicted: 3.48, Confidence: 57.7 - MODERATE)\n",
      "  4. #5 LIAM IN THE DUST (Predicted: 3.70, Confidence: 68.3 - MODERATE)\n",
      "  Spread: 0.01, Avg Top-2 Confidence: 70.7\n",
      "----------------------------------------\n",
      "\n",
      "Race 7:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #1 GOLD LUCK (Predicted: 4.21, Confidence: 17.9 - LOW)\n",
      "  2. #7 CAT ON TIME (Predicted: 4.96, Confidence: 66.7 - MODERATE)\n",
      "  3. #5 FORT SAM (Predicted: 5.83, Confidence: 66.3 - MODERATE)\n",
      "  4. #3 BOHEMIAN BO (Predicted: 6.22, Confidence: 71.5 - MODERATE)\n",
      "  Spread: 0.75, Avg Top-2 Confidence: 42.3\n",
      "----------------------------------------\n",
      "\n",
      "Race 8:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #5 OPERATION SUNRISE (Predicted: 4.09, Confidence: 35.6 - LOW)\n",
      "  2. #14 SHADOW COAST (Predicted: 4.73, Confidence: 3.7 - LOW)\n",
      "  3. #10 MO WORK (Predicted: 6.29, Confidence: 79.8 - HIGH)\n",
      "  4. #9 NO AH A (Predicted: 6.39, Confidence: 79.7 - HIGH)\n",
      "  Spread: 0.64, Avg Top-2 Confidence: 19.7\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display current race predictions with confidence scores\n",
    "print(f\"Predictions for {track_name} on {race_date}:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df.columns = ['race_id', 'horse_id', 'predicted_finish_position', 'confidence_score']\n",
    "results_df['predicted_finish_position'] = results_df['predicted_finish_position'].round(2)\n",
    "results_df['confidence_score'] = results_df['confidence_score'].round(1)\n",
    "\n",
    "# Group by race and show top picks with confidence\n",
    "for race_id in results_df['race_id'].unique():\n",
    "    race_data = results_df[results_df['race_id'] == race_id].sort_values('predicted_finish_position')\n",
    "    race_num = race_id.split('_')[1]\n",
    "    print(f\"\\nRace {race_num}:\")\n",
    "    print(\"Top 4 Predictions (with confidence):\")\n",
    "    for i, (_, row) in enumerate(race_data.head(4).iterrows(), 1):\n",
    "        horse_name = row['horse_id'].split('_')[0]\n",
    "        prog_num = row['horse_id'].split('_')[1] \n",
    "        confidence = row['confidence_score']\n",
    "        confidence_level = \"HIGH\" if confidence >= 75 else \"MODERATE\" if confidence >= 50 else \"LOW\"\n",
    "        print(f\"  {i}. #{prog_num} {horse_name} (Predicted: {row['predicted_finish_position']:.2f}, Confidence: {confidence:.1f} - {confidence_level})\")\n",
    "    \n",
    "    # Calculate spread between top two picks\n",
    "    if len(race_data) >= 2:\n",
    "        top_two = race_data.head(2)\n",
    "        spread = top_two.iloc[1]['predicted_finish_position'] - top_two.iloc[0]['predicted_finish_position']\n",
    "        avg_confidence = top_two['confidence_score'].mean()\n",
    "        print(f\"  Spread: {spread:.2f}, Avg Top-2 Confidence: {avg_confidence:.1f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Variance Analysis & Multi-Race Betting Strategies\n",
    "\n",
    "To develop effective multi-race betting strategies, we need to analyze the historical variance between our model's top predictions and actual race winners. This analysis will help us understand the reliability of our predictions and develop strategies that account for uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical data for variance analysis...\n",
      "Loaded 4675 historical race entries\n",
      "Covering 575 historical races\n"
     ]
    }
   ],
   "source": [
    "# Load historical training data to analyze prediction variance\n",
    "print(\"Loading historical data for variance analysis...\")\n",
    "\n",
    "# Load the imputed data that was used for training\n",
    "historical_data = pd.read_csv(f'Imputed Data\\\\{track_name}.csv')\n",
    "print(f\"Loaded {len(historical_data)} historical race entries\")\n",
    "\n",
    "# Get unique race count\n",
    "unique_races = historical_data['race_id'].nunique()\n",
    "print(f\"Covering {unique_races} historical races\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on historical data...\n",
      "Generated predictions for 4675 historical entries\n"
     ]
    }
   ],
   "source": [
    "# Recreate predictions on historical data to analyze variance\n",
    "print(\"Generating predictions on historical data...\")\n",
    "\n",
    "# Prepare the historical data similar to how we prepared the current race data\n",
    "historical_features = historical_data.drop(columns=['normalized_position', 'Position', 'odds'])\n",
    "historical_actual = historical_data['Position'].astype(int)\n",
    "\n",
    "# Get predictions for historical data using ensemble if available\n",
    "if use_ensemble:\n",
    "    # Generate ensemble predictions on historical data\n",
    "    historical_ensemble_predictions = []\n",
    "    for ensemble_model in ensemble_models:\n",
    "        predictions = ensemble_model.predict(historical_features[feature_names])\n",
    "        historical_ensemble_predictions.append(predictions)\n",
    "    \n",
    "    # Convert to numpy array and calculate mean predictions\n",
    "    historical_ensemble_predictions = np.array(historical_ensemble_predictions)\n",
    "    historical_predicted_normalized = np.mean(historical_ensemble_predictions, axis=0)\n",
    "else:\n",
    "    # Single model prediction\n",
    "    historical_predicted_normalized = model.predict(historical_features[feature_names])\n",
    "\n",
    "# Convert normalized predictions to actual positions\n",
    "historical_predicted_positions = (historical_predicted_normalized * historical_features['number_of_run']) / 100\n",
    "\n",
    "# Create variance analysis DataFrame\n",
    "variance_df = pd.DataFrame({\n",
    "    'race_id': historical_data['race_id'],\n",
    "    'actual_position': historical_actual,\n",
    "    'predicted_position': historical_predicted_positions,\n",
    "    'prediction_error': historical_predicted_positions - historical_actual,\n",
    "    'number_of_run': historical_features['number_of_run']\n",
    "})\n",
    "\n",
    "print(f\"Generated predictions for {len(variance_df)} historical entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION VARIANCE ANALYSIS\n",
      "==================================================\n",
      "Top Pick Performance:\n",
      "  Win Rate: 73.0%\n",
      "  Top 3 Rate: 91.0%\n",
      "  Average Actual Finish: 1.65\n",
      "  Average Prediction Error: 1.31 positions\n",
      "  Standard Deviation of Error: 1.47 positions\n",
      "\n",
      "Prediction Error Distribution:\n",
      "  95% of top picks finish within 3.19 positions of prediction\n",
      "  5% of top picks finish within -1.18 positions of prediction\n",
      "  68% confidence interval: ¬±1.47 positions\n",
      "  95% confidence interval: ¬±2.88 positions\n"
     ]
    }
   ],
   "source": [
    "# Analyze top pick variance\n",
    "print(\"PREDICTION VARIANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# For each race, get the top predicted horse and analyze its actual performance\n",
    "top_picks_analysis = []\n",
    "\n",
    "for race_id in variance_df['race_id'].unique():\n",
    "    race_data = variance_df[variance_df['race_id'] == race_id]\n",
    "    \n",
    "    # Find the horse with the best (lowest) predicted position\n",
    "    top_pick_idx = race_data['predicted_position'].idxmin()\n",
    "    top_pick = race_data.loc[top_pick_idx]\n",
    "    \n",
    "    top_picks_analysis.append({\n",
    "        'race_id': race_id,\n",
    "        'predicted_position': top_pick['predicted_position'],\n",
    "        'actual_position': top_pick['actual_position'],\n",
    "        'prediction_error': top_pick['prediction_error'],\n",
    "        'field_size': top_pick['number_of_run'],\n",
    "        'won_race': top_pick['actual_position'] == 1,\n",
    "        'in_top_3': top_pick['actual_position'] <= 3,\n",
    "        'finished_worse_than_predicted': top_pick['actual_position'] > top_pick['predicted_position']\n",
    "    })\n",
    "\n",
    "top_picks_df = pd.DataFrame(top_picks_analysis)\n",
    "\n",
    "# Calculate key statistics\n",
    "win_rate = top_picks_df['won_race'].mean() * 100\n",
    "top_3_rate = top_picks_df['in_top_3'].mean() * 100\n",
    "avg_prediction_error = top_picks_df['prediction_error'].mean()\n",
    "std_prediction_error = top_picks_df['prediction_error'].std()\n",
    "avg_actual_position = top_picks_df['actual_position'].mean()\n",
    "\n",
    "print(f\"Top Pick Performance:\")\n",
    "print(f\"  Win Rate: {win_rate:.1f}%\")\n",
    "print(f\"  Top 3 Rate: {top_3_rate:.1f}%\")\n",
    "print(f\"  Average Actual Finish: {avg_actual_position:.2f}\")\n",
    "print(f\"  Average Prediction Error: {avg_prediction_error:.2f} positions\")\n",
    "print(f\"  Standard Deviation of Error: {std_prediction_error:.2f} positions\")\n",
    "\n",
    "# Calculate confidence intervals\n",
    "error_95th_percentile = np.percentile(top_picks_df['prediction_error'], 95)\n",
    "error_5th_percentile = np.percentile(top_picks_df['prediction_error'], 5)\n",
    "\n",
    "print(f\"\\nPrediction Error Distribution:\")\n",
    "print(f\"  95% of top picks finish within {error_95th_percentile:.2f} positions of prediction\")\n",
    "print(f\"  5% of top picks finish within {error_5th_percentile:.2f} positions of prediction\")\n",
    "print(f\"  68% confidence interval: ¬±{std_prediction_error:.2f} positions\")\n",
    "print(f\"  95% confidence interval: ¬±{1.96 * std_prediction_error:.2f} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions saved to: predictions_cd_20250626.csv\n",
      "üìä Data saved: 80 entries for 8 races\n",
      "üèÅ Track: Churchill Downs\n",
      "üìÖ Date: 20250626\n",
      "\n",
      "üéØ Next step: Open BetBuilder.ipynb to build betting strategies using this data.\n"
     ]
    }
   ],
   "source": [
    "# Save prediction results to CSV for BetBuilder.ipynb\n",
    "import os\n",
    "\n",
    "# Create filename with track and date\n",
    "csv_filename = f\"Predictions\\\\predictions_{track_abbreviation}_{race_date}.csv\"\n",
    "csv_path = os.path.join(os.getcwd(), csv_filename)\n",
    "\n",
    "# Save results_df to CSV\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Predictions saved to: {csv_filename}\")\n",
    "print(f\"üìä Data saved: {len(results_df)} entries for {len(results_df['race_id'].unique())} races\")\n",
    "print(f\"üèÅ Track: {track_name}\")\n",
    "print(f\"üìÖ Date: {race_date}\")\n",
    "print(f\"\\nüéØ Next step: Open BetBuilder.ipynb to build betting strategies using this data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
