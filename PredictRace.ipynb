{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_abbreviation = \"cd\"\n",
    "track_name = \"Churchill Downs\"\n",
    "race_date = \"20250626\"\n",
    "scratches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_quality_dict = {\n",
    "    \"TRL\": 1,\n",
    "    \"MCL\": 1,\n",
    "    \"WMC\": 1,\n",
    "    \"MOC\": 1.5,\n",
    "    \"MSA\": 1.5,\n",
    "    \"MSW\": 2.5,\n",
    "    \"WCL\": 2,\n",
    "    \"CLM\": 2,\n",
    "    \"MST\": 2.5,\n",
    "    \"CLH\": 2.5,\n",
    "    \"CST\": 2.5,\n",
    "    \"SOC\": 2.5,\n",
    "    \"OCL\":\t2.75,\n",
    "    \"SHP\":\t3,\n",
    "    \"STR\":\t2.75,\n",
    "    \"AOC\": 3.25,\n",
    "    \"OCS\": 3.5,\n",
    "    \"OCH\":\t3.25,\n",
    "    \"ALW\":\t4,\n",
    "    \"HCP\":\t4,\n",
    "    \"SIM\":\t2,\n",
    "    \"SST\":\t3.5,\n",
    "    \"STK\": 5\n",
    "}\n",
    "\n",
    "race_types = {\n",
    "    \"AL\": \"ALW\",\n",
    "    \"MS\": \"MSW\",\n",
    "    \"CL\": \"CLM\",\n",
    "    \"OC\": \"AOC\",\n",
    "    \"MC\": \"MCL\",\n",
    "    \"SO\": \"SOC\",\n",
    "    \"MO\": \"MCL\", # MO is Maiden Optional Claiming but the simulcast data contains no definition for it\n",
    "    \"ST\": \"STK\",\n",
    "    \"SA\": \"STR\"\n",
    "}\n",
    "\n",
    "equipment = {\n",
    "    \"B\": \"B\",\n",
    "    \"F\": \"None\",\n",
    "    \"BF\": \"B\",\n",
    "    \"V\": \"V\",\n",
    "    \"R\": \"R\",\n",
    "    \"Y\": \"Y\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PP data for:  asd20250625ppsXML.xml\n",
      "Ohtani OHTANI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 2: Load and Parse XML Data using pandas.read_xml\n",
    "def load_performance_data(file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    print(\"Loading PP data for: \", os.path.basename(file_path))\n",
    "    \n",
    "    # Extract each Race element within EntryRaceCard and convert to a dictionary\n",
    "    races = []\n",
    "    for race in root.findall('.//racedata'):\n",
    "        race_dict = xmltodict.parse(ET.tostring(race))['racedata']\n",
    "        race_date = race_dict['race_date']\n",
    "        track_name = race_dict['track']\n",
    "\n",
    "        race_dict = extract_general_race_info(race_dict, race_date, track_name)\n",
    "\n",
    "        num_scratches = 0\n",
    "        for entry in race.findall('.//horsedata'):\n",
    "            entry_dict = extract_entry_info(entry, race_date, num_scratches)\n",
    "            workout_dict = extract_workout_info(entry, race_date)\n",
    "            \n",
    "            is_scratched = False\n",
    "            for horse in scratches:\n",
    "                if horse.lower() in entry_dict['horse_id'].split('_')[0].lower():\n",
    "                    print(horse, entry_dict['horse_id'].split('_')[0])\n",
    "                    is_scratched = True\n",
    "                    num_scratches += 1\n",
    "                    break\n",
    "            if not is_scratched:\n",
    "                races.append({**race_dict, **entry_dict, **workout_dict})\n",
    "        \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(races)\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_general_race_info(race_dict, race_date, track_name):\n",
    "    race_id = f\"{race_date}_{race_dict['race']}_{track_name}\"\n",
    "    return {\n",
    "        \"race_id\": race_id,\n",
    "        \"course_type\": str(race_dict['surface']),\n",
    "        \"distance\": int(float(race_dict['distance'])),\n",
    "        \"race_type\": race_types[str(race_dict['stkorclm'])],\n",
    "        \"restriction_type\": \"S\" if \"state\" in str(race_dict['race_text']).lower() else \"None\",\n",
    "        \"purse\": float(race_dict['purse']),\n",
    "        \"number_of_run\": len(race_dict['horsedata']),\n",
    "    }\n",
    "\n",
    "def extract_entry_info(entry_root, race_date, scratches):\n",
    "    entry_dict = xmltodict.parse(ET.tostring(entry_root))['horsedata']\n",
    "\n",
    "    final_dict = {\n",
    "        \"horse_id\": f\"{entry_dict['horse_name']}_{entry_dict['program']}\",\n",
    "        \"gender\": str(entry_dict['sex']),\n",
    "        \"post_position\": int(entry_dict['pp']) - scratches,\n",
    "        \"weight\": int(entry_dict['weight']),\n",
    "        \"equipment\": str(entry_dict['equip']),\n",
    "        \"medication\": \"L\" if str(entry_dict['med']) == \"1\" else \"None\",\n",
    "        \"jockey_win_percentage\": float(entry_dict['jockey']['stats_data']['stat']['wins']) / float(entry_dict['jockey']['stats_data']['stat']['starts']) if float(entry_dict['jockey']['stats_data']['stat']['starts']) != 0 else 0,\n",
    "        \"trainer_win_percentage\": float(entry_dict['trainer']['stats_data']['stat']['wins']) / float(entry_dict['trainer']['stats_data']['stat']['starts']) if float(entry_dict['trainer']['stats_data']['stat']['starts']) != 0 else 0,\n",
    "        \"trainer_jockey_win_percentage\": float(entry_dict['stats_data']['stat'][22]['wins']) / float(entry_dict['stats_data']['stat'][22]['starts']) if float(entry_dict['stats_data']['stat'][22]['starts']) != 0 else 0,\n",
    "    }\n",
    "\n",
    "    summaries = entry_root.findall('.//stats_data')\n",
    "    if summaries is list:\n",
    "        summary_dict = xmltodict.parse(ET.tostring(summaries[0]))['stat']['THIS_YEAR']\n",
    "        final_dict.update({\n",
    "            \"win_percentage_year\": float(summary_dict['wins']) / float(summary_dict['starts']),\n",
    "            \"otb_percentage_year\": (float(summary_dict['wins']) + float(summary_dict['places']) + float(summary_dict['shows'])) / float(summary_dict['starts']),\n",
    "        })\n",
    "\n",
    "    ignored_scratches = 0\n",
    "    for i, pp in enumerate(entry_root.findall('.//ppdata')):\n",
    "        normalized_i = i - ignored_scratches\n",
    "        if normalized_i > 5:\n",
    "            break\n",
    "        pp_dict = xmltodict.parse(ET.tostring(pp))['ppdata']\n",
    "\n",
    "        # Get the actual equipment being worn\n",
    "        if normalized_i == 0:\n",
    "            if final_dict['equipment'] != \"OFF\":\n",
    "                if str(pp_dict['equipment']) in equipment.keys():\n",
    "                    final_dict.update({\n",
    "                        \"equipment\": equipment[str(pp_dict['equipment'])]\n",
    "                    })\n",
    "                else:\n",
    "                    final_dict.update({\n",
    "                        \"equipment\": \"None\"\n",
    "                    })\n",
    "            else:\n",
    "                final_dict.update({\n",
    "                    \"equipment\": \"O\"\n",
    "                })\n",
    "\n",
    "        race_type = str(pp_dict['racetype'])\n",
    "        if (race_type == 'SCR'):\n",
    "            ignored_scratches += 1\n",
    "            continue\n",
    "        race_quality = race_quality_dict[race_type] if str(race_type) in race_quality_dict.keys() else 1\n",
    "\n",
    "        if str(pp_dict['statebredr']) == 'S':\n",
    "            race_quality -= 1\n",
    "        if race_type == 'STK' and pp_dict['racegrade'] != 0:\n",
    "            race_quality += 1 + int(pp_dict['racegrade'])\n",
    "\n",
    "        if i == 0:\n",
    "            final_dict.update({\n",
    "                \"pp_layoff\": (datetime.strptime(race_date, '%Y%m%d') - datetime.strptime(pp_dict['racedate'][:10], '%Y%m%d')).days\n",
    "            })\n",
    "\n",
    "        bad_luck = False\n",
    "        long_comment = str(pp_dict['longcommen']).lower()\n",
    "        if long_comment is not None:\n",
    "            if any(['bump' in long_comment, 'stumbled' in long_comment, 'checked' in long_comment, 'steadied' in long_comment, 'stopped' in long_comment, 'squeezed' in long_comment, 'steady' in long_comment or 'steadied' in long_comment, 'head turned' in long_comment, 'unprepared start' in long_comment, 'wd' in long_comment or 'wide' in long_comment, 'bled' in long_comment]):\n",
    "                bad_luck = True\n",
    "\n",
    "        final_dict.update({\n",
    "            f\"pp_track_{normalized_i}\": str(pp_dict['trackcode']),\n",
    "            f\"pp_time_since_race_{normalized_i}\": (datetime.strptime(race_date, '%Y%m%d') - datetime.strptime(pp_dict['racedate'][:10], '%Y%m%d')).days,\n",
    "            f\"pp_course_type_{normalized_i}\": str(pp_dict['surface']),\n",
    "            f\"pp_distance_{normalized_i}\": int(pp_dict['distance']),\n",
    "            f\"pp_quality_{normalized_i}\": race_quality,\n",
    "            f\"pp_purse_{normalized_i}\": float(pp_dict['purse']),\n",
    "            f\"pp_normalized_position_{normalized_i}\": np.divide(float(pp_dict['positionfi']), float(pp_dict['fieldsize'])),\n",
    "            f\"pp_class_rating_{normalized_i}\": int(pp_dict['classratin']),\n",
    "            f\"pp_speed_rating_{normalized_i}\": int(pp_dict['speedfigur']),\n",
    "            f\"pp_pace_figure_{normalized_i}\": int(pp_dict['pacefigur2']),\n",
    "            f\"pp_bad_luck_{normalized_i}\": bad_luck,\n",
    "        })\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def extract_workout_info(entry_root, race_date):\n",
    "    final_dict = {}\n",
    "    for i, workout in enumerate(entry_root.findall('.//workoutdata')):    \n",
    "        if i > 3:\n",
    "            return final_dict\n",
    "        workout_dict = xmltodict.parse(ET.tostring(workout))['workoutdata']\n",
    "        final_dict.update({\n",
    "            f\"workout_last_month_{i}\": True if int(workout_dict['days_back']) < 30 else False,\n",
    "            f\"workout_distance_{i}\": int(workout_dict['worktext'][0]) * 100,\n",
    "            f\"workout_course_type_{i}\": \"D\", # No data available, usually dirt\n",
    "            f\"workout_time_{i}\": int(float(re.sub('\\D', '', get_substring_from_char(workout_dict['worktext'], ':'))) * 10),\n",
    "            f\"workout_rank_{i}\": int(workout_dict['ranking']) / int(workout_dict['rank_group']),\n",
    "        })\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def get_substring_from_char(s, char):\n",
    "    pos = s.find(char) + 1\n",
    "    if pos != -1:\n",
    "        return s[pos:]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "performance_path = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\XML_PPs\"\n",
    "file_name = f'{track_abbreviation}{race_date}ppsXML.xml'  # Add your suffixes here\n",
    "\n",
    "# Load all past performance files\n",
    "performance_data = load_performance_data(performance_path + '\\\\' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_21960\\3537981293.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  performance_data.at[i, \"first_off_layoff\"] = False\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_21960\\3537981293.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  performance_data.at[i, \"second_off_layoff\"] = False\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_21960\\3537981293.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  performance_data.at[i, \"third_off_layoff\"] = False\n"
     ]
    }
   ],
   "source": [
    "# Go through each row and create new columns\n",
    "for i, row in performance_data.iterrows():\n",
    "    # Initialize new columns\n",
    "    performance_data.at[i, \"first_off_layoff\"] = False\n",
    "    performance_data.at[i, \"second_off_layoff\"] = False\n",
    "    performance_data.at[i, \"third_off_layoff\"] = False\n",
    "    \n",
    "    # Set if horse is off layoff\n",
    "    if row['pp_time_since_race_0'] > 45:\n",
    "        performance_data.at[i, 'first_off_layoff'] = True\n",
    "    elif row['pp_time_since_race_1'] - row['pp_time_since_race_0'] > 45:\n",
    "        performance_data.at[i, 'second_off_layoff'] = True\n",
    "    elif row['pp_time_since_race_2'] - row['pp_time_since_race_1'] > 45:\n",
    "        performance_data.at[i, 'third_off_layoff'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "performance_data.reset_index()\n",
    "\n",
    "# Drop unneeded columns\n",
    "horse_ids = performance_data['horse_id']\n",
    "performance_data = performance_data.drop(columns=['horse_id'])\n",
    "\n",
    "# Impute only columns with missing values\n",
    "imputer: SimpleImputer = pickle.load(open(f\"Models\\\\{track_name}\\\\imputer.pkl\", \"rb\"))\n",
    "columns_with_missing = imputer.feature_names_in_\n",
    "imputed_array = imputer.transform(performance_data[columns_with_missing])\n",
    "\n",
    "# Convert the imputed array back to a DataFrame with original column names\n",
    "imputed_data = pd.DataFrame(imputed_array, columns=columns_with_missing)\n",
    "\n",
    "# Combine the imputed columns with the rest of the data\n",
    "data = performance_data.copy()\n",
    "data[columns_with_missing] = imputed_data\n",
    "\n",
    "# Get correct column types\n",
    "data = data.infer_objects()\n",
    "\n",
    "# Use LabelEncoder on string columns\n",
    "label_encoders = pickle.load(open(f\"Models\\\\{track_name}\\\\label_encoders.pkl\", \"rb\"))\n",
    "for col in data.columns:\n",
    "    if col == \"race_id\" or col == \"horse_id\":\n",
    "        data[col] = LabelEncoder().fit_transform(data[col])\n",
    "    elif data[col].dtype == 'object':\n",
    "        try:\n",
    "            if ('pp_track' in col):\n",
    "                # If the track was unknown in test, set it to be the same as the track we're running at\n",
    "                data[col] = data[col].map(lambda s: track_abbreviation.upper() if s not in label_encoders[col].classes_ else s)\n",
    "            data[col] = label_encoders[col].transform(data[col])\n",
    "        except:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col])\n",
    "            print(f\"Error encoding column: {col}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ensemble of 5 models\n",
      "Ensemble predictions generated with confidence scores\n",
      "Average confidence: 50.16\n",
      "Confidence range: 0.00 - 100.00\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open(f\"Models\\\\{track_name}\\\\{track_name}_Model.pkl\", \"rb\"))\n",
    "\n",
    "feature_names = model.feature_names_in_\n",
    "\n",
    "# Reorder the columns of merged_data_imputed to match the order of feature_names\n",
    "data = data[feature_names]\n",
    "\n",
    "# Try to load ensemble models first, fallback to single model\n",
    "try:\n",
    "    ensemble_models = pickle.load(open(f\"Models\\\\{track_name}\\\\{track_name}_Ensemble.pkl\", \"rb\"))\n",
    "    print(f\"Loaded ensemble of {len(ensemble_models)} models\")\n",
    "    use_ensemble = True\n",
    "    model = ensemble_models[0]  # Keep for compatibility\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensemble models not found, using single model\")\n",
    "    model = pickle.load(open(f\"Models\\\\{track_name}\\\\{track_name}_Model.pkl\", \"rb\"))\n",
    "    use_ensemble = False\n",
    "\n",
    "feature_names = model.feature_names_in_\n",
    "\n",
    "# Reorder the columns of merged_data_imputed to match the order of feature_names\n",
    "data = data[feature_names]\n",
    "\n",
    "# Generate predictions\n",
    "if use_ensemble:\n",
    "    # Generate predictions from all models in the ensemble\n",
    "    ensemble_predictions = []\n",
    "    for i, ensemble_model in enumerate(ensemble_models):\n",
    "        predictions = ensemble_model.predict(data)\n",
    "        ensemble_predictions.append(predictions)\n",
    "    \n",
    "    # Convert to numpy array for easier manipulation\n",
    "    ensemble_predictions = np.array(ensemble_predictions)\n",
    "    \n",
    "    # Calculate ensemble statistics\n",
    "    y_predict = np.mean(ensemble_predictions, axis=0)  # Mean prediction\n",
    "    prediction_std = np.std(ensemble_predictions, axis=0)  # Standard deviation across models\n",
    "    \n",
    "    # Calculate confidence scores based on model agreement (inverse of standard deviation)\n",
    "    # Lower std = higher confidence, higher std = lower confidence\n",
    "    max_std = np.max(prediction_std)\n",
    "    min_std = np.min(prediction_std)\n",
    "    \n",
    "    # Normalize confidence to 0-100 scale (100 = highest confidence, 0 = lowest confidence)\n",
    "    if max_std > min_std:\n",
    "        confidence_scores = 100 * (1 - (prediction_std - min_std) / (max_std - min_std))\n",
    "    else:\n",
    "        confidence_scores = np.full_like(prediction_std, 100)  # All predictions have same confidence\n",
    "    \n",
    "    print(f\"Ensemble predictions generated with confidence scores\")\n",
    "    print(f\"Average confidence: {np.mean(confidence_scores):.2f}\")\n",
    "    print(f\"Confidence range: {np.min(confidence_scores):.2f} - {np.max(confidence_scores):.2f}\")\n",
    "    \n",
    "else:\n",
    "    # Single model prediction\n",
    "    y_predict = model.predict(data)\n",
    "    confidence_scores = np.full_like(y_predict, 50)  # Default moderate confidence\n",
    "    print(\"Single model predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated for 43 horses\n",
      "Confidence scores range: 0.0 to 100.0\n"
     ]
    }
   ],
   "source": [
    "# Use the ensemble predictions from the previous cell\n",
    "predicted_normalized_position = y_predict\n",
    "predicted_finish_position = ((predicted_normalized_position * data['number_of_run']) / 100)\n",
    "\n",
    "# Add confidence scores to the results\n",
    "print(f\"Predictions generated for {len(predicted_finish_position)} horses\")\n",
    "print(f\"Confidence scores range: {np.min(confidence_scores):.1f} to {np.max(confidence_scores):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame by concatenating the series, now including confidence scores\n",
    "results_df = pd.concat([performance_data['race_id'], horse_ids, predicted_finish_position, pd.Series(confidence_scores, index=performance_data.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Assiniboia Downs on 20250625:\n",
      "============================================================\n",
      "\n",
      "Race 1:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #4 SMACKOVER LIME (Predicted: 3.60, Confidence: 57.9 - MODERATE)\n",
      "  2. #6 IN FORM (Predicted: 3.78, Confidence: 50.2 - MODERATE)\n",
      "  3. #1 SALTO DEL LUPO (Predicted: 4.12, Confidence: 19.1 - LOW)\n",
      "  4. #5 LUCKY APPEAL (Predicted: 4.16, Confidence: 40.6 - LOW)\n",
      "  Spread: 0.18, Avg Top-2 Confidence: 54.0\n",
      "----------------------------------------\n",
      "\n",
      "Race 2:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #4 GINAS SERENADE (Predicted: 2.00, Confidence: 62.6 - MODERATE)\n",
      "  2. #3 WELCOMETOHOLLYWOOD (Predicted: 2.28, Confidence: 34.9 - LOW)\n",
      "  3. #2 MCGILL (Predicted: 2.48, Confidence: 71.6 - MODERATE)\n",
      "  4. #1 LINDA'S MAP (Predicted: 2.76, Confidence: 19.3 - LOW)\n",
      "  Spread: 0.28, Avg Top-2 Confidence: 48.8\n",
      "----------------------------------------\n",
      "\n",
      "Race 3:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #3 MECHANIC SUSIE (Predicted: 3.19, Confidence: 34.8 - LOW)\n",
      "  2. #4 SOPHIA'S STORM (Predicted: 3.19, Confidence: 13.6 - LOW)\n",
      "  3. #1 DISTINCTIVE DEB (Predicted: 3.21, Confidence: 30.8 - LOW)\n",
      "  4. #6 BURROW DOWN (Predicted: 3.56, Confidence: 78.8 - HIGH)\n",
      "  Spread: 0.00, Avg Top-2 Confidence: 24.2\n",
      "----------------------------------------\n",
      "\n",
      "Race 4:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #3 LT. NORM (Predicted: 3.63, Confidence: 0.0 - LOW)\n",
      "  2. #5 PLUNGEINTOTHENITE (Predicted: 3.75, Confidence: 25.9 - LOW)\n",
      "  3. #7 WAR ECHO (Predicted: 4.08, Confidence: 16.7 - LOW)\n",
      "  4. #6 TIZ LORD (Predicted: 4.28, Confidence: 55.3 - MODERATE)\n",
      "  Spread: 0.12, Avg Top-2 Confidence: 12.9\n",
      "----------------------------------------\n",
      "\n",
      "Race 5:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #2 DRAGON DREW (Predicted: 4.12, Confidence: 31.5 - LOW)\n",
      "  2. #3 MAIBELLA (Predicted: 4.27, Confidence: 38.2 - LOW)\n",
      "  3. #4 PRAIRIE DRIFTER (Predicted: 4.28, Confidence: 67.6 - MODERATE)\n",
      "  4. #5 DIANE'S WISH (Predicted: 4.37, Confidence: 57.2 - MODERATE)\n",
      "  Spread: 0.15, Avg Top-2 Confidence: 34.9\n",
      "----------------------------------------\n",
      "\n",
      "Race 6:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #2 DIAMOND GRAND (Predicted: 3.51, Confidence: 18.4 - LOW)\n",
      "  2. #6 STEVIE WONDER GIRL (Predicted: 4.17, Confidence: 56.7 - MODERATE)\n",
      "  3. #7 SAVAGE LOVE (Predicted: 4.27, Confidence: 62.9 - MODERATE)\n",
      "  4. #3 HIGHLANDER HONEY (Predicted: 4.41, Confidence: 48.6 - LOW)\n",
      "  Spread: 0.66, Avg Top-2 Confidence: 37.5\n",
      "----------------------------------------\n",
      "\n",
      "Race 7:\n",
      "Top 4 Predictions (with confidence):\n",
      "  1. #7 DUSK (Predicted: 4.04, Confidence: 100.0 - HIGH)\n",
      "  2. #4 AWESOME WILDCAT (Predicted: 4.29, Confidence: 45.6 - LOW)\n",
      "  3. #5 OFFLEY SPECIAL (Predicted: 4.37, Confidence: 29.0 - LOW)\n",
      "  4. #1 BLUE TANGO (Predicted: 4.46, Confidence: 72.3 - MODERATE)\n",
      "  Spread: 0.25, Avg Top-2 Confidence: 72.8\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display current race predictions with confidence scores\n",
    "print(f\"Predictions for {track_name} on {race_date}:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df.columns = ['race_id', 'horse_id', 'predicted_finish_position', 'confidence_score']\n",
    "results_df['predicted_finish_position'] = results_df['predicted_finish_position'].round(2)\n",
    "results_df['confidence_score'] = results_df['confidence_score'].round(1)\n",
    "\n",
    "# Group by race and show top picks with confidence\n",
    "for race_id in results_df['race_id'].unique():\n",
    "    race_data = results_df[results_df['race_id'] == race_id].sort_values('predicted_finish_position')\n",
    "    race_num = race_id.split('_')[1]\n",
    "    print(f\"\\nRace {race_num}:\")\n",
    "    print(\"Top 4 Predictions (with confidence):\")\n",
    "    for i, (_, row) in enumerate(race_data.head(4).iterrows(), 1):\n",
    "        horse_name = row['horse_id'].split('_')[0]\n",
    "        prog_num = row['horse_id'].split('_')[1] \n",
    "        confidence = row['confidence_score']\n",
    "        confidence_level = \"HIGH\" if confidence >= 75 else \"MODERATE\" if confidence >= 50 else \"LOW\"\n",
    "        print(f\"  {i}. #{prog_num} {horse_name} (Predicted: {row['predicted_finish_position']:.2f}, Confidence: {confidence:.1f} - {confidence_level})\")\n",
    "    \n",
    "    # Calculate spread between top two picks\n",
    "    if len(race_data) >= 2:\n",
    "        top_two = race_data.head(2)\n",
    "        spread = top_two.iloc[1]['predicted_finish_position'] - top_two.iloc[0]['predicted_finish_position']\n",
    "        avg_confidence = top_two['confidence_score'].mean()\n",
    "        print(f\"  Spread: {spread:.2f}, Avg Top-2 Confidence: {avg_confidence:.1f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Variance Analysis & Multi-Race Betting Strategies\n",
    "\n",
    "To develop effective multi-race betting strategies, we need to analyze the historical variance between our model's top predictions and actual race winners. This analysis will help us understand the reliability of our predictions and develop strategies that account for uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical data for variance analysis...\n",
      "Loaded 2334 historical race entries\n",
      "Covering 350 historical races\n"
     ]
    }
   ],
   "source": [
    "# Load historical training data to analyze prediction variance\n",
    "print(\"Loading historical data for variance analysis...\")\n",
    "\n",
    "# Load the imputed data that was used for training\n",
    "historical_data = pd.read_csv(f'Imputed Data\\\\{track_name}.csv')\n",
    "print(f\"Loaded {len(historical_data)} historical race entries\")\n",
    "\n",
    "# Get unique race count\n",
    "unique_races = historical_data['race_id'].nunique()\n",
    "print(f\"Covering {unique_races} historical races\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on historical data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated predictions for 2334 historical entries\n"
     ]
    }
   ],
   "source": [
    "# Recreate predictions on historical data to analyze variance\n",
    "print(\"Generating predictions on historical data...\")\n",
    "\n",
    "# Prepare the historical data similar to how we prepared the current race data\n",
    "historical_features = historical_data.drop(columns=['normalized_position', 'Position', 'odds'])\n",
    "historical_actual = historical_data['Position'].astype(int)\n",
    "\n",
    "# Get predictions for historical data using ensemble if available\n",
    "if use_ensemble:\n",
    "    # Generate ensemble predictions on historical data\n",
    "    historical_ensemble_predictions = []\n",
    "    for ensemble_model in ensemble_models:\n",
    "        predictions = ensemble_model.predict(historical_features[feature_names])\n",
    "        historical_ensemble_predictions.append(predictions)\n",
    "    \n",
    "    # Convert to numpy array and calculate mean predictions\n",
    "    historical_ensemble_predictions = np.array(historical_ensemble_predictions)\n",
    "    historical_predicted_normalized = np.mean(historical_ensemble_predictions, axis=0)\n",
    "else:\n",
    "    # Single model prediction\n",
    "    historical_predicted_normalized = model.predict(historical_features[feature_names])\n",
    "\n",
    "# Convert normalized predictions to actual positions\n",
    "historical_predicted_positions = (historical_predicted_normalized * historical_features['number_of_run']) / 100\n",
    "\n",
    "# Create variance analysis DataFrame\n",
    "variance_df = pd.DataFrame({\n",
    "    'race_id': historical_data['race_id'],\n",
    "    'actual_position': historical_actual,\n",
    "    'predicted_position': historical_predicted_positions,\n",
    "    'prediction_error': historical_predicted_positions - historical_actual,\n",
    "    'number_of_run': historical_features['number_of_run']\n",
    "})\n",
    "\n",
    "print(f\"Generated predictions for {len(variance_df)} historical entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION VARIANCE ANALYSIS\n",
      "==================================================\n",
      "Top Pick Performance:\n",
      "  Win Rate: 70.0%\n",
      "  Top 3 Rate: 92.9%\n",
      "  Average Actual Finish: 1.61\n",
      "  Average Prediction Error: 1.14 positions\n",
      "  Standard Deviation of Error: 1.20 positions\n",
      "\n",
      "Prediction Error Distribution:\n",
      "  95% of top picks finish within 2.65 positions of prediction\n",
      "  5% of top picks finish within -1.54 positions of prediction\n",
      "  68% confidence interval: ±1.20 positions\n",
      "  95% confidence interval: ±2.36 positions\n"
     ]
    }
   ],
   "source": [
    "# Analyze top pick variance\n",
    "print(\"PREDICTION VARIANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# For each race, get the top predicted horse and analyze its actual performance\n",
    "top_picks_analysis = []\n",
    "\n",
    "for race_id in variance_df['race_id'].unique():\n",
    "    race_data = variance_df[variance_df['race_id'] == race_id]\n",
    "    \n",
    "    # Find the horse with the best (lowest) predicted position\n",
    "    top_pick_idx = race_data['predicted_position'].idxmin()\n",
    "    top_pick = race_data.loc[top_pick_idx]\n",
    "    \n",
    "    top_picks_analysis.append({\n",
    "        'race_id': race_id,\n",
    "        'predicted_position': top_pick['predicted_position'],\n",
    "        'actual_position': top_pick['actual_position'],\n",
    "        'prediction_error': top_pick['prediction_error'],\n",
    "        'field_size': top_pick['number_of_run'],\n",
    "        'won_race': top_pick['actual_position'] == 1,\n",
    "        'in_top_3': top_pick['actual_position'] <= 3,\n",
    "        'finished_worse_than_predicted': top_pick['actual_position'] > top_pick['predicted_position']\n",
    "    })\n",
    "\n",
    "top_picks_df = pd.DataFrame(top_picks_analysis)\n",
    "\n",
    "# Calculate key statistics\n",
    "win_rate = top_picks_df['won_race'].mean() * 100\n",
    "top_3_rate = top_picks_df['in_top_3'].mean() * 100\n",
    "avg_prediction_error = top_picks_df['prediction_error'].mean()\n",
    "std_prediction_error = top_picks_df['prediction_error'].std()\n",
    "avg_actual_position = top_picks_df['actual_position'].mean()\n",
    "\n",
    "print(f\"Top Pick Performance:\")\n",
    "print(f\"  Win Rate: {win_rate:.1f}%\")\n",
    "print(f\"  Top 3 Rate: {top_3_rate:.1f}%\")\n",
    "print(f\"  Average Actual Finish: {avg_actual_position:.2f}\")\n",
    "print(f\"  Average Prediction Error: {avg_prediction_error:.2f} positions\")\n",
    "print(f\"  Standard Deviation of Error: {std_prediction_error:.2f} positions\")\n",
    "\n",
    "# Calculate confidence intervals\n",
    "error_95th_percentile = np.percentile(top_picks_df['prediction_error'], 95)\n",
    "error_5th_percentile = np.percentile(top_picks_df['prediction_error'], 5)\n",
    "\n",
    "print(f\"\\nPrediction Error Distribution:\")\n",
    "print(f\"  95% of top picks finish within {error_95th_percentile:.2f} positions of prediction\")\n",
    "print(f\"  5% of top picks finish within {error_5th_percentile:.2f} positions of prediction\")\n",
    "print(f\"  68% confidence interval: ±{std_prediction_error:.2f} positions\")\n",
    "print(f\"  95% confidence interval: ±{1.96 * std_prediction_error:.2f} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved to: predictions_asd_20250625.csv\n",
      "📊 Data saved: 43 entries for 7 races\n",
      "🏁 Track: Assiniboia Downs\n",
      "📅 Date: 20250625\n",
      "\n",
      "🎯 Next step: Open BetBuilder.ipynb to build betting strategies using this data.\n"
     ]
    }
   ],
   "source": [
    "# Save prediction results to CSV for BetBuilder.ipynb\n",
    "import os\n",
    "\n",
    "# Create filename with track and date\n",
    "csv_filename = f\"predictions_{track_abbreviation}_{race_date}.csv\"\n",
    "csv_path = os.path.join(os.getcwd(), csv_filename)\n",
    "\n",
    "# Save results_df to CSV\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Predictions saved to: {csv_filename}\")\n",
    "print(f\"📊 Data saved: {len(results_df)} entries for {len(results_df['race_id'].unique())} races\")\n",
    "print(f\"🏁 Track: {track_name}\")\n",
    "print(f\"📅 Date: {race_date}\")\n",
    "print(f\"\\n🎯 Next step: Open BetBuilder.ipynb to build betting strategies using this data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
