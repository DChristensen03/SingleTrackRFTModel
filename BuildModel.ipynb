{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_abbreviation = \"asd\"\n",
    "track_name = \"Assiniboia Downs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(f'Imputed Data\\\\{track_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble of 5 Random Forest models...\n",
      "Training model 1/5...\n",
      "Training model 2/5...\n",
      "Training model 2/5...\n",
      "Training model 3/5...\n",
      "Training model 3/5...\n",
      "Training model 4/5...\n",
      "Training model 4/5...\n",
      "Training model 5/5...\n",
      "Training model 5/5...\n",
      "Ensemble training complete!\n",
      "Ensemble training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'race_type' is a column in your data\n",
    "X = data.drop(columns=['normalized_position', 'Position'])\n",
    "y = data['normalized_position']\n",
    "groups = data['race_id']\n",
    "stratify_col = data['race_type']\n",
    "\n",
    "# Initialize GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create empty lists to store train and test indices\n",
    "train_idx = []\n",
    "test_idx = []\n",
    "\n",
    "# Split each race_type group\n",
    "for race_type in stratify_col.unique():\n",
    "    race_type_mask = stratify_col == race_type\n",
    "    X_race_type = X[race_type_mask]\n",
    "    y_race_type = y[race_type_mask]\n",
    "    groups_race_type = groups[race_type_mask]\n",
    "    \n",
    "    if len(X_race_type) < 2:\n",
    "        continue\n",
    "    \n",
    "    if len(X_race_type) > 1 and len(y_race_type) > 1:  # Ensure there are enough samples to split\n",
    "        gss_split = gss.split(X_race_type, y_race_type, groups=groups_race_type)\n",
    "        try:\n",
    "            train_idx_race_type, test_idx_race_type = next(gss_split)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "        train_idx.extend(X_race_type.index[train_idx_race_type])\n",
    "        test_idx.extend(X_race_type.index[test_idx_race_type])\n",
    "\n",
    "# Convert lists to arrays\n",
    "train_idx = pd.Index(train_idx)\n",
    "test_idx = pd.Index(test_idx)\n",
    "\n",
    "X_train, X_test = X.loc[train_idx], X.loc[test_idx]\n",
    "y_train, y_test = y.loc[train_idx], y.loc[test_idx]\n",
    "\n",
    "# Store the 'odds' column for later use\n",
    "odds_train = X_train['odds']\n",
    "odds_test = X_test['odds']\n",
    "\n",
    "# Drop the 'odds' column from the training and testing datasets\n",
    "X_train = X_train.drop(columns=['odds'])\n",
    "X_test = X_test.drop(columns=['odds'])\n",
    "\n",
    "# Create ensemble of random forest models with different configurations\n",
    "ensemble_models = []\n",
    "model_configs = [\n",
    "    {'n_estimators': 300, 'max_depth': 20, 'max_features': 0.5, 'min_samples_leaf': 10, 'random_state': 42},\n",
    "    {'n_estimators': 250, 'max_depth': 25, 'max_features': 0.6, 'min_samples_leaf': 8, 'random_state': 123},\n",
    "    {'n_estimators': 350, 'max_depth': 15, 'max_features': 0.4, 'min_samples_leaf': 12, 'random_state': 456},\n",
    "    {'n_estimators': 275, 'max_depth': 30, 'max_features': 0.7, 'min_samples_leaf': 15, 'random_state': 789},\n",
    "    {'n_estimators': 320, 'max_depth': 18, 'max_features': 0.5, 'min_samples_leaf': 9, 'random_state': 999}\n",
    "]\n",
    "\n",
    "print(f\"Training ensemble of {len(model_configs)} Random Forest models...\")\n",
    "\n",
    "for i, config in enumerate(model_configs):\n",
    "    print(f\"Training model {i+1}/{len(model_configs)}...\")\n",
    "    model = RandomForestRegressor(n_jobs=-1, **config)\n",
    "    model.fit(X_train, y_train)\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "print(\"Ensemble training complete!\")\n",
    "\n",
    "# Make predictions with each model in the ensemble\n",
    "predictions = np.zeros((len(X_test), len(ensemble_models)))\n",
    "\n",
    "for i, model in enumerate(ensemble_models):\n",
    "    predictions[:, i] = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean prediction and the standard deviation (as a measure of uncertainty)\n",
    "y_pred_mean = predictions.mean(axis=1)\n",
    "y_pred_std = predictions.std(axis=1)\n",
    "\n",
    "# Calculate confidence scores (e.g., using the inverse of the standard deviation)\n",
    "confidence_scores = 1 / (1 + y_pred_std)\n",
    "\n",
    "# The final predictions can be the mean predictions, and you can use the confidence scores as needed\n",
    "final_predictions = y_pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 predictions generated\n",
      "Model 2 predictions generated\n",
      "Model 3 predictions generated\n",
      "Model 4 predictions generated\n",
      "Model 5 predictions generated\n",
      "Ensemble predictions complete!\n",
      "Average confidence score: 65.61\n",
      "Confidence score range: 0.00 - 100.00\n",
      "Model 5 predictions generated\n",
      "Ensemble predictions complete!\n",
      "Average confidence score: 65.61\n",
      "Confidence score range: 0.00 - 100.00\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions from all models in the ensemble\n",
    "ensemble_predictions = []\n",
    "for i, model in enumerate(ensemble_models):\n",
    "    predictions = model.predict(X_test)\n",
    "    ensemble_predictions.append(predictions)\n",
    "    print(f\"Model {i+1} predictions generated\")\n",
    "\n",
    "# Convert to numpy array for easier manipulation\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "\n",
    "# Calculate ensemble statistics\n",
    "predicted_normalized_position = np.mean(ensemble_predictions, axis=0)  # Mean prediction\n",
    "prediction_std = np.std(ensemble_predictions, axis=0)  # Standard deviation across models\n",
    "\n",
    "# Calculate confidence scores based on model agreement (inverse of standard deviation)\n",
    "# Lower std = higher confidence, higher std = lower confidence\n",
    "max_std = np.max(prediction_std)\n",
    "min_std = np.min(prediction_std)\n",
    "\n",
    "# Normalize confidence to 0-100 scale (100 = highest confidence, 0 = lowest confidence)\n",
    "if max_std > min_std:\n",
    "    confidence_scores = 100 * (1 - (prediction_std - min_std) / (max_std - min_std))\n",
    "else:\n",
    "    confidence_scores = np.full_like(prediction_std, 100)  # All predictions have same confidence\n",
    "\n",
    "# Convert to finish positions\n",
    "predicted_finish_position = ((predicted_normalized_position * X_test['number_of_run']) / 100)\n",
    "\n",
    "print(f\"Ensemble predictions complete!\")\n",
    "print(f\"Average confidence score: {np.mean(confidence_scores):.2f}\")\n",
    "print(f\"Confidence score range: {np.min(confidence_scores):.2f} - {np.max(confidence_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract actual finish positions\n",
    "actual_finish_position = (y_test * X_test['number_of_run']) / 100\n",
    "actual_finish_position = actual_finish_position.astype(int)\n",
    "\n",
    "# Extract and normalize the Odds column\n",
    "odds = odds_test\n",
    "normalized_odds = (odds - odds.min()) / (odds.max() - odds.min()) * 100  # Scale to a range of 0 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results dataframe created with 474 predictions\n",
      "Sample confidence scores: [75.35058077 68.97879056 50.50699063 77.39271927 66.67451125]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame by concatenating the series, now including confidence scores\n",
    "results_df = pd.concat([X_test['race_id'], actual_finish_position, predicted_finish_position, odds, pd.Series(confidence_scores, index=X_test.index)], axis=1)\n",
    "\n",
    "# Rename the columns for clarity\n",
    "results_df.columns = ['race_id', 'actual_finish_position', 'predicted_finish_position', 'odds', 'confidence_score']\n",
    "\n",
    "print(f\"Results dataframe created with {len(results_df)} predictions\")\n",
    "print(f\"Sample confidence scores: {results_df['confidence_score'].head().values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe from results_df that selects the lowest predicted_finish_position for each race_id\n",
    "best_predictions = results_df.groupby('race_id').agg({'predicted_finish_position': 'min'}).reset_index()\n",
    "# Merge the best_predictions dataframe with results_df to get all other columns\n",
    "best_predictions = pd.merge(best_predictions, results_df, on=[\"race_id\", \"predicted_finish_position\"], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 36.99%\n",
      "Place: 60.27%\n",
      "Show: 75.34%\n"
     ]
    }
   ],
   "source": [
    "# Calculate percent of horses that finished 1\n",
    "num_firsts = best_predictions[best_predictions['actual_finish_position'] == 1].shape[0]\n",
    "print(\"Win: {:.2f}%\".format(num_firsts / best_predictions.shape[0] * 100))\n",
    "\n",
    "# Calculate percent of horses that finished 1 or 2\n",
    "num_seconds = best_predictions[best_predictions['actual_finish_position'] <= 2].shape[0]\n",
    "print(\"Place: {:.2f}%\".format(num_seconds / best_predictions.shape[0] * 100))\n",
    "\n",
    "# Calculate percent of horses that finished 1, 2, or 3\n",
    "num_thirds = best_predictions[best_predictions['actual_finish_position'] <= 3].shape[0]\n",
    "print(\"Show: {:.2f}%\".format(num_thirds / best_predictions.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROI per bet: 48.08%\n"
     ]
    }
   ],
   "source": [
    "# Generate win bet ROI per race\n",
    "grouped = best_predictions.groupby('race_id')\n",
    "roi_per_race = grouped.apply(lambda x: (x[x['actual_finish_position'] == 1]['odds'].sum() - len(x)) / len(x) * 100)\n",
    "\n",
    "# Calculate average ROI per bet\n",
    "average_roi_per_bet = roi_per_race.mean()\n",
    "\n",
    "# Print average ROI per bet\n",
    "print(f\"Average ROI per bet: {average_roi_per_bet:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 5 models saved to Models\\Assiniboia Downs\\Assiniboia Downs_Ensemble.pkl\n",
      "Primary model saved to Models\\Assiniboia Downs\\Assiniboia Downs_Model.pkl\n",
      "Primary model saved to Models\\Assiniboia Downs\\Assiniboia Downs_Model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the entire ensemble of models\n",
    "ensemble_model_file = f\"Models\\\\{track_name}\\\\{track_name}_Ensemble.pkl\"\n",
    "primary_model_file = f\"Models\\\\{track_name}\\\\{track_name}_Model.pkl\"\n",
    "\n",
    "# Save ensemble models\n",
    "with open(ensemble_model_file, 'wb') as file:  \n",
    "    pickle.dump(ensemble_models, file)\n",
    "    \n",
    "print(f\"Ensemble of {len(ensemble_models)} models saved to {ensemble_model_file}\")\n",
    "\n",
    "# Also save the primary model for backward compatibility\n",
    "with open(primary_model_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n",
    "    \n",
    "print(f\"Primary model saved to {primary_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of times where the top 2 predictions both finished 1 and 2 in any order: 21.92%\n"
     ]
    }
   ],
   "source": [
    "# Get the top two predictions for each race_id\n",
    "top_two_predictions = results_df.groupby('race_id').apply(lambda x: x.nsmallest(2, 'predicted_finish_position')).reset_index(drop=True)\n",
    "\n",
    "# Check if both top two predictions finished in the top two positions\n",
    "top_two_grouped = top_two_predictions.groupby('race_id').filter(lambda x: set(x['actual_finish_position']) == {1, 2})\n",
    "\n",
    "# Calculate the percentage of races where both top two predictions finished in the top two positions\n",
    "percentage_top_two = (top_two_grouped['race_id'].nunique() / results_df['race_id'].nunique()) * 100\n",
    "\n",
    "print(\"Percentage of times where the top 2 predictions both finished 1 and 2 in any order: {:.2f}%\".format(percentage_top_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of times where the top 3 predictions all finished 1, 2, and 3 in any order: 2.74%\n"
     ]
    }
   ],
   "source": [
    "# Find the top 3 horses in each race_id\n",
    "top_three_predictions = results_df.groupby('race_id').apply(lambda x: x.nsmallest(3, 'predicted_finish_position')).reset_index(drop=True)\n",
    "\n",
    "# Check if all top three predictions finished in the top three positions\n",
    "top_three_grouped = top_three_predictions.groupby('race_id').filter(lambda x: set(x['actual_finish_position']) == {1, 2, 3})\n",
    "\n",
    "# Calculate the percentage of races where both top three predictions finished in the top three positions\n",
    "percentage_top_three = (top_three_grouped['race_id'].nunique() / results_df['race_id'].nunique()) * 100\n",
    "\n",
    "print(\"Percentage of times where the top 3 predictions all finished 1, 2, and 3 in any order: {:.2f}%\".format(percentage_top_three))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of times where the top 4 predictions all finished 1, 2, 3, and 4 in any order: 53.42%\n"
     ]
    }
   ],
   "source": [
    "# Find the top 4 horses in each race_id\n",
    "top_four_predictions = results_df.groupby('race_id').apply(lambda x: x.nsmallest(4, 'predicted_finish_position')).reset_index(drop=True)\n",
    "\n",
    "# Check if three of the top four predictions finished in the top three positions\n",
    "top_four_grouped = top_four_predictions.query('actual_finish_position <= 3')\n",
    "\n",
    "# Find the number of race_ids that occur 3 times in top_four_grouped\n",
    "race_id_counts = top_four_grouped['race_id'].value_counts()\n",
    "race_ids_with_three_occurrences = len(race_id_counts[race_id_counts == 3])\n",
    "percentage_top_four_trifecta = (race_ids_with_three_occurrences / top_four_predictions.groupby('race_id').ngroups) * 100\n",
    "\n",
    "print(\"Percentage of times where the top 4 predictions all finished 1, 2, 3, and 4 in any order: {:.2f}%\".format(percentage_top_four_trifecta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OPTIMAL HORSE COVERAGE ANALYSIS FOR >80% SUCCESS RATE\n",
      "======================================================================\n",
      "\n",
      "Low Confidence (0-40%): 7 races, Avg: 29.9\n",
      "  Success rates: 1H:28.6% | 2H:42.9% | 3H:0.0%\n",
      "  ⚠️  No coverage level achieves 80% success rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Medium Confidence (40-60%): 34 races, Avg: 51.6\n",
      "  Success rates: 1H:23.5% | 2H:41.2% | 3H:63.2% | 4H:75.0% | 5H:66.7% | 6H:0.0%\n",
      "  ⚠️  No coverage level achieves 80% success rate\n",
      "\n",
      "High Confidence (60-75%): 44 races, Avg: 68.3\n",
      "  Success rates: 1H:15.9% | 2H:31.8% | 3H:41.4% | 4H:47.1% | 5H:50.0% | 6H:33.3%\n",
      "  ⚠️  No coverage level achieves 80% success rate\n",
      "\n",
      "Very High Confidence (75-90%): 47 races, Avg: 81.6\n",
      "Very High Confidence (75-90%): 47 races, Avg: 81.6\n",
      "  Success rates: 1H:25.5% | 2H:29.8% | 3H:53.8% | 4H:42.9% | 5H:40.0%\n",
      "  ⚠️  No coverage level achieves 80% success rate\n",
      "\n",
      "Extreme Confidence (90-100%): 2 races, Avg: 93.6\n",
      "  Success rates: 1H:0.0% | 2H:0.0%\n",
      "  ⚠️  No coverage level achieves 80% success rate\n",
      "\n",
      "📊 Coverage analysis saved to Models\\Assiniboia Downs\\Assiniboia Downs_Coverage_Analysis.pkl\n",
      "This data will be used in BetBuilder for dynamic horse selection\n",
      "\n",
      "📋 CONFIDENCE-BASED HORSE SELECTION LOOKUP TABLE:\n",
      "------------------------------------------------------------\n",
      "Low          (  0-40%): No reliable strategy\n",
      "Medium       ( 40-60%): No reliable strategy\n",
      "High         ( 60-75%): No reliable strategy\n",
      "Very High    ( 75-90%): No reliable strategy\n",
      "Extreme      (90-100%): No reliable strategy\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Success rates: 1H:25.5% | 2H:29.8% | 3H:53.8% | 4H:42.9% | 5H:40.0%\n",
      "  ⚠️  No coverage level achieves 80% success rate\n",
      "\n",
      "Extreme Confidence (90-100%): 2 races, Avg: 93.6\n",
      "  Success rates: 1H:0.0% | 2H:0.0%\n",
      "  ⚠️  No coverage level achieves 80% success rate\n",
      "\n",
      "📊 Coverage analysis saved to Models\\Assiniboia Downs\\Assiniboia Downs_Coverage_Analysis.pkl\n",
      "This data will be used in BetBuilder for dynamic horse selection\n",
      "\n",
      "📋 CONFIDENCE-BASED HORSE SELECTION LOOKUP TABLE:\n",
      "------------------------------------------------------------\n",
      "Low          (  0-40%): No reliable strategy\n",
      "Medium       ( 40-60%): No reliable strategy\n",
      "High         ( 60-75%): No reliable strategy\n",
      "Very High    ( 75-90%): No reliable strategy\n",
      "Extreme      (90-100%): No reliable strategy\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Calculate optimal horse coverage for >80% success rate by confidence level\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMAL HORSE COVERAGE ANALYSIS FOR >80% SUCCESS RATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_optimal_coverage_by_confidence():\n",
    "    \"\"\"\n",
    "    Calculate how many horses are needed to achieve >80% success rate for different confidence levels\n",
    "    \"\"\"\n",
    "    # Create confidence bins\n",
    "    confidence_bins = [\n",
    "        (0, 40, \"Low\"),\n",
    "        (40, 60, \"Medium\"), \n",
    "        (60, 75, \"High\"),\n",
    "        (75, 90, \"Very High\"),\n",
    "        (90, 100, \"Extreme\")\n",
    "    ]\n",
    "    \n",
    "    coverage_analysis = {}\n",
    "    \n",
    "    for min_conf, max_conf, label in confidence_bins:\n",
    "        # Filter races by confidence level\n",
    "        conf_mask = (results_df['confidence_score'] >= min_conf) & (results_df['confidence_score'] < max_conf)\n",
    "        conf_data = results_df[conf_mask]\n",
    "        \n",
    "        if len(conf_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # For each race, test different horse coverage levels\n",
    "        race_coverage_results = []\n",
    "        \n",
    "        for race_id in conf_data['race_id'].unique():\n",
    "            race_data = conf_data[conf_data['race_id'] == race_id].sort_values('predicted_finish_position')\n",
    "            \n",
    "            if len(race_data) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Test coverage from 1 to min(field_size, 6) horses\n",
    "            max_horses = min(len(race_data), 6)\n",
    "            race_results = {'race_id': race_id, 'field_size': len(race_data)}\n",
    "            \n",
    "            for num_horses in range(1, max_horses + 1):\n",
    "                top_horses = race_data.head(num_horses)\n",
    "                winner_covered = (top_horses['actual_finish_position'] == 1).any()\n",
    "                race_results[f'covered_{num_horses}'] = winner_covered\n",
    "                \n",
    "            race_coverage_results.append(race_results)\n",
    "        \n",
    "        if not race_coverage_results:\n",
    "            continue\n",
    "            \n",
    "        # Calculate success rates for each coverage level\n",
    "        coverage_df = pd.DataFrame(race_coverage_results)\n",
    "        total_races = len(coverage_df)\n",
    "        \n",
    "        coverage_stats = {}\n",
    "        optimal_horses = None\n",
    "        \n",
    "        for num_horses in range(1, 7):\n",
    "            col_name = f'covered_{num_horses}'\n",
    "            if col_name in coverage_df.columns:\n",
    "                success_rate = coverage_df[col_name].mean() * 100\n",
    "                coverage_stats[num_horses] = success_rate\n",
    "                \n",
    "                # Find minimum horses needed for >80% success rate\n",
    "                if success_rate >= 80 and optimal_horses is None:\n",
    "                    optimal_horses = num_horses\n",
    "        \n",
    "        coverage_analysis[label] = {\n",
    "            'confidence_range': f\"{min_conf}-{max_conf}\",\n",
    "            'total_races': total_races,\n",
    "            'coverage_stats': coverage_stats,\n",
    "            'optimal_horses': optimal_horses,\n",
    "            'avg_confidence': conf_data['confidence_score'].mean()\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n{label} Confidence ({min_conf}-{max_conf}%): {total_races} races, Avg: {conf_data['confidence_score'].mean():.1f}\")\n",
    "        success_rates = []\n",
    "        for h in range(1, 7):\n",
    "            if h in coverage_stats:\n",
    "                rate = coverage_stats[h]\n",
    "                marker = \" ✅\" if rate >= 80 else \"\"\n",
    "                success_rates.append(f\"{h}H:{rate:.1f}%{marker}\")\n",
    "        print(f\"  Success rates: {' | '.join(success_rates)}\")\n",
    "        \n",
    "        if optimal_horses:\n",
    "            print(f\"  🎯 OPTIMAL: Use {optimal_horses} horses for >80% success rate\")\n",
    "        else:\n",
    "            print(f\"  ⚠️  No coverage level achieves 80% success rate\")\n",
    "    \n",
    "    return coverage_analysis\n",
    "\n",
    "# Run the analysis\n",
    "optimal_coverage_by_confidence = calculate_optimal_coverage_by_confidence()\n",
    "\n",
    "# Save the coverage analysis for use in BetBuilder\n",
    "import pickle\n",
    "coverage_file = f\"Models\\\\{track_name}\\\\{track_name}_Coverage_Analysis.pkl\"\n",
    "with open(coverage_file, 'wb') as file:\n",
    "    pickle.dump(optimal_coverage_by_confidence, file)\n",
    "    \n",
    "print(f\"\\n📊 Coverage analysis saved to {coverage_file}\")\n",
    "print(\"This data will be used in BetBuilder for dynamic horse selection\")\n",
    "\n",
    "# Create a summary lookup table\n",
    "print(f\"\\n📋 CONFIDENCE-BASED HORSE SELECTION LOOKUP TABLE:\")\n",
    "print(\"-\" * 60)\n",
    "for label, data in optimal_coverage_by_confidence.items():\n",
    "    optimal = data['optimal_horses']\n",
    "    if optimal:\n",
    "        print(f\"{label:12} ({data['confidence_range']:>6}%): Use {optimal} horses\")\n",
    "    else:\n",
    "        print(f\"{label:12} ({data['confidence_range']:>6}%): No reliable strategy\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONFIDENCE SCORE ANALYSIS\n",
      "============================================================\n",
      "                    Total_Predictions  Wins  Avg_Predicted_Position  \\\n",
      "confidence_bin                                                        \n",
      "Low (0-25)                         10     2                    3.10   \n",
      "Medium (25-50)                     70    13                    3.73   \n",
      "High (50-75)                      244    35                    4.11   \n",
      "Very High (75-100)                149    21                    4.23   \n",
      "\n",
      "                    Win_Rate_%  \n",
      "confidence_bin                  \n",
      "Low (0-25)               20.00  \n",
      "Medium (25-50)           18.57  \n",
      "High (50-75)             14.34  \n",
      "Very High (75-100)       14.09  \n",
      "\n",
      "Average Prediction Error by Confidence Level:\n",
      "  Low (0-25): 1.44 positions\n",
      "  Medium (25-50): 1.86 positions\n",
      "  High (50-75): 1.74 positions\n",
      "  Very High (75-100): 1.63 positions\n",
      "\n",
      "Correlation between confidence and accuracy: 0.040\n",
      "(Higher values indicate confidence scores are well-calibrated)\n",
      "\n",
      "Best Picks Analysis with Confidence:\n",
      "Average confidence of best picks: 62.06\n",
      "High confidence (≥75) best picks win rate: 42.11% (8/19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_10840\\1066204927.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  confidence_analysis = results_df.groupby('confidence_bin').agg({\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_10840\\1066204927.py:24: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  error_by_confidence = results_df.groupby('confidence_bin')['prediction_error'].mean().round(2)\n"
     ]
    }
   ],
   "source": [
    "# Analyze confidence score performance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIDENCE SCORE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create confidence bins\n",
    "results_df['confidence_bin'] = pd.cut(results_df['confidence_score'], \n",
    "                                    bins=[0, 25, 50, 75, 100], \n",
    "                                    labels=['Low (0-25)', 'Medium (25-50)', 'High (50-75)', 'Very High (75-100)'])\n",
    "\n",
    "# Analyze accuracy by confidence level\n",
    "confidence_analysis = results_df.groupby('confidence_bin').agg({\n",
    "    'actual_finish_position': ['count', lambda x: (x == 1).sum()],\n",
    "    'predicted_finish_position': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "confidence_analysis.columns = ['Total_Predictions', 'Wins', 'Avg_Predicted_Position']\n",
    "confidence_analysis['Win_Rate_%'] = (confidence_analysis['Wins'] / confidence_analysis['Total_Predictions'] * 100).round(2)\n",
    "\n",
    "print(confidence_analysis)\n",
    "\n",
    "# Calculate prediction error by confidence level\n",
    "results_df['prediction_error'] = abs(results_df['actual_finish_position'] - results_df['predicted_finish_position'])\n",
    "error_by_confidence = results_df.groupby('confidence_bin')['prediction_error'].mean().round(2)\n",
    "\n",
    "print(f\"\\nAverage Prediction Error by Confidence Level:\")\n",
    "for bin_name, error in error_by_confidence.items():\n",
    "    print(f\"  {bin_name}: {error:.2f} positions\")\n",
    "\n",
    "# Show correlation between confidence and accuracy\n",
    "correlation = results_df['confidence_score'].corr(-results_df['prediction_error'])\n",
    "print(f\"\\nCorrelation between confidence and accuracy: {correlation:.3f}\")\n",
    "print(\"(Higher values indicate confidence scores are well-calibrated)\")\n",
    "\n",
    "# Best picks analysis with confidence\n",
    "best_predictions_with_confidence = results_df.groupby('race_id').apply(\n",
    "    lambda x: x.loc[x['predicted_finish_position'].idxmin()]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nBest Picks Analysis with Confidence:\")\n",
    "print(f\"Average confidence of best picks: {best_predictions_with_confidence['confidence_score'].mean():.2f}\")\n",
    "\n",
    "high_conf_wins = best_predictions_with_confidence[\n",
    "    (best_predictions_with_confidence['confidence_score'] >= 75) & \n",
    "    (best_predictions_with_confidence['actual_finish_position'] == 1)\n",
    "].shape[0]\n",
    "\n",
    "high_conf_total = best_predictions_with_confidence[\n",
    "    best_predictions_with_confidence['confidence_score'] >= 75\n",
    "].shape[0]\n",
    "\n",
    "if high_conf_total > 0:\n",
    "    print(f\"High confidence (≥75) best picks win rate: {high_conf_wins/high_conf_total*100:.2f}% ({high_conf_wins}/{high_conf_total})\")\n",
    "else:\n",
    "    print(\"No high confidence best picks found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
